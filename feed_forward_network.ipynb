{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('kaggle/california_housing_prices.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "# Drop rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Use one-hot encoding for categorical variables\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('median_house_value', axis=1).values\n",
    "y = df['median_house_value'].values\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        # create a neural network with 2 hidden layers\n",
    "        # the first hidden layer has 128 neurons\n",
    "        # the second hidden layer has 64 neurons\n",
    "        # the output layer has 1 neuron\n",
    "        # use ReLU activation function for hidden layers\n",
    "        self.fc1 = nn.Linear(input_size, 128) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply ReLU activation function to the first hidden layer\n",
    "        x = self.relu(self.fc1(x)) \n",
    "        # apply ReLU activation function to the second hidden layer\n",
    "        x = self.relu(self.fc2(x)) \n",
    "        # output layer\n",
    "        x = self.fc3(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 55938727936.0\n",
      "Epoch 11/1000, Loss: 55927013376.0\n",
      "Epoch 21/1000, Loss: 55869984768.0\n",
      "Epoch 31/1000, Loss: 55700086784.0\n",
      "Epoch 41/1000, Loss: 55307309056.0\n",
      "Epoch 51/1000, Loss: 54540169216.0\n",
      "Epoch 61/1000, Loss: 53219205120.0\n",
      "Epoch 71/1000, Loss: 51162669056.0\n",
      "Epoch 81/1000, Loss: 48223932416.0\n",
      "Epoch 91/1000, Loss: 44338892800.0\n",
      "Epoch 101/1000, Loss: 39574777856.0\n",
      "Epoch 111/1000, Loss: 34170204160.0\n",
      "Epoch 121/1000, Loss: 28527054848.0\n",
      "Epoch 131/1000, Loss: 23093557248.0\n",
      "Epoch 141/1000, Loss: 18263402496.0\n",
      "Epoch 151/1000, Loss: 14409444352.0\n",
      "Epoch 161/1000, Loss: 11783067648.0\n",
      "Epoch 171/1000, Loss: 10299244544.0\n",
      "Epoch 181/1000, Loss: 9545753600.0\n",
      "Epoch 191/1000, Loss: 9090859008.0\n",
      "Epoch 201/1000, Loss: 8722791424.0\n",
      "Epoch 211/1000, Loss: 8399208960.0\n",
      "Epoch 221/1000, Loss: 8118205952.0\n",
      "Epoch 231/1000, Loss: 7872098304.0\n",
      "Epoch 241/1000, Loss: 7651426816.0\n",
      "Epoch 251/1000, Loss: 7449597952.0\n",
      "Epoch 261/1000, Loss: 7262687744.0\n",
      "Epoch 271/1000, Loss: 7088117248.0\n",
      "Epoch 281/1000, Loss: 6924363264.0\n",
      "Epoch 291/1000, Loss: 6770992128.0\n",
      "Epoch 301/1000, Loss: 6627402752.0\n",
      "Epoch 311/1000, Loss: 6492709376.0\n",
      "Epoch 321/1000, Loss: 6366516224.0\n",
      "Epoch 331/1000, Loss: 6248680960.0\n",
      "Epoch 341/1000, Loss: 6138750464.0\n",
      "Epoch 351/1000, Loss: 6036110848.0\n",
      "Epoch 361/1000, Loss: 5940051456.0\n",
      "Epoch 371/1000, Loss: 5850364928.0\n",
      "Epoch 381/1000, Loss: 5766547968.0\n",
      "Epoch 391/1000, Loss: 5688167424.0\n",
      "Epoch 401/1000, Loss: 5614951424.0\n",
      "Epoch 411/1000, Loss: 5546412544.0\n",
      "Epoch 421/1000, Loss: 5482295296.0\n",
      "Epoch 431/1000, Loss: 5422286336.0\n",
      "Epoch 441/1000, Loss: 5366289408.0\n",
      "Epoch 451/1000, Loss: 5313953280.0\n",
      "Epoch 461/1000, Loss: 5265157632.0\n",
      "Epoch 471/1000, Loss: 5219559424.0\n",
      "Epoch 481/1000, Loss: 5176965120.0\n",
      "Epoch 491/1000, Loss: 5137161216.0\n",
      "Epoch 501/1000, Loss: 5099884544.0\n",
      "Epoch 511/1000, Loss: 5065098240.0\n",
      "Epoch 521/1000, Loss: 5032647680.0\n",
      "Epoch 531/1000, Loss: 5002258944.0\n",
      "Epoch 541/1000, Loss: 4973726208.0\n",
      "Epoch 551/1000, Loss: 4946810880.0\n",
      "Epoch 561/1000, Loss: 4921415680.0\n",
      "Epoch 571/1000, Loss: 4897445376.0\n",
      "Epoch 581/1000, Loss: 4874818048.0\n",
      "Epoch 591/1000, Loss: 4853440512.0\n",
      "Epoch 601/1000, Loss: 4833178112.0\n",
      "Epoch 611/1000, Loss: 4813963776.0\n",
      "Epoch 621/1000, Loss: 4795750912.0\n",
      "Epoch 631/1000, Loss: 4778485760.0\n",
      "Epoch 641/1000, Loss: 4762115584.0\n",
      "Epoch 651/1000, Loss: 4746544128.0\n",
      "Epoch 661/1000, Loss: 4731673600.0\n",
      "Epoch 671/1000, Loss: 4717484032.0\n",
      "Epoch 681/1000, Loss: 4703886336.0\n",
      "Epoch 691/1000, Loss: 4690836480.0\n",
      "Epoch 701/1000, Loss: 4678319616.0\n",
      "Epoch 711/1000, Loss: 4666259456.0\n",
      "Epoch 721/1000, Loss: 4654610944.0\n",
      "Epoch 731/1000, Loss: 4643325952.0\n",
      "Epoch 741/1000, Loss: 4632346624.0\n",
      "Epoch 751/1000, Loss: 4621648896.0\n",
      "Epoch 761/1000, Loss: 4611190272.0\n",
      "Epoch 771/1000, Loss: 4600939008.0\n",
      "Epoch 781/1000, Loss: 4590861824.0\n",
      "Epoch 791/1000, Loss: 4580937216.0\n",
      "Epoch 801/1000, Loss: 4571150336.0\n",
      "Epoch 811/1000, Loss: 4561518080.0\n",
      "Epoch 821/1000, Loss: 4552063488.0\n",
      "Epoch 831/1000, Loss: 4542756352.0\n",
      "Epoch 841/1000, Loss: 4533630976.0\n",
      "Epoch 851/1000, Loss: 4524757504.0\n",
      "Epoch 861/1000, Loss: 4516090368.0\n",
      "Epoch 871/1000, Loss: 4507569152.0\n",
      "Epoch 881/1000, Loss: 4499195904.0\n",
      "Epoch 891/1000, Loss: 4490972672.0\n",
      "Epoch 901/1000, Loss: 4482894336.0\n",
      "Epoch 911/1000, Loss: 4475017728.0\n",
      "Epoch 921/1000, Loss: 4467290112.0\n",
      "Epoch 931/1000, Loss: 4459719168.0\n",
      "Epoch 941/1000, Loss: 4452286464.0\n",
      "Epoch 951/1000, Loss: 4444958720.0\n",
      "Epoch 961/1000, Loss: 4437709312.0\n",
      "Epoch 971/1000, Loss: 4430545408.0\n",
      "Epoch 981/1000, Loss: 4423429120.0\n",
      "Epoch 991/1000, Loss: 4416302592.0\n"
     ]
    }
   ],
   "source": [
    "# Convert arrays to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train).view(-1, 1)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test).view(-1, 1)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = RegressionNN(X_train.shape[1]) # input_size is the number of features\n",
    "criterion = nn.MSELoss() # Mean Squared Error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) # Adam is a variant of gradient descent, lr is the learning rate\n",
    "\n",
    "# Training loop\n",
    "epochs = 1_000\n",
    "for epoch in range(epochs):\n",
    "    model.train() # Set the model to training mode\n",
    "    optimizer.zero_grad() # Reset gradients\n",
    "    outputs = model(X_train_tensor) # Forward pass\n",
    "    loss = criterion(outputs, y_train_tensor) # Compute loss\n",
    "    loss.backward() # Backward pass\n",
    "    optimizer.step() # Update weights\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 4520810496.0\n"
     ]
    }
   ],
   "source": [
    "model.eval() # Set the model to evaluation mode\n",
    "with torch.no_grad(): # Turn off gradient computation for validation to save time\n",
    "    predictions = model(X_test_tensor) # Make predictions\n",
    "    mse = criterion(predictions, y_test_tensor) # Calculate the loss\n",
    "\n",
    "print(f'Test MSE: {mse.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
